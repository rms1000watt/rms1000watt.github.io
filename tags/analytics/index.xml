<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Analytics on rms1000watt</title>
    <link>https://rms1000watt.github.io/tags/analytics/</link>
    <description>Recent content in Analytics on rms1000watt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jul 2017 08:30:00 -0800</lastBuildDate>
    
	<atom:link href="https://rms1000watt.github.io/tags/analytics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Analytics Cheat Sheet</title>
      <link>https://rms1000watt.github.io/post/remedial-analytics/</link>
      <pubDate>Sat, 15 Jul 2017 08:30:00 -0800</pubDate>
      
      <guid>https://rms1000watt.github.io/post/remedial-analytics/</guid>
      <description>Linear vs Nonlinear Linear if:
$$f(x + y) = f(x) + f(y)$$ $$f(\alpha x) = \alpha f(x)$$
Linear Algebra Dot Product $$\vec{a}\cdot\vec{b} = a_{1}b_{1} + a_{2}b_{2} + ... = |\vec{a}||\vec{b}|\cos{\theta} $$ Projection The projection of $\vec{b}$ onto $\vec{a}$ is:
$$ proj_{\vec{a}}\vec{b} = \frac{\vec{a}\cdot\vec{b}}{|\vec{a}|^2}\vec{a} $$ Orthogonal Matrix A matrix $Q$ is orthogonal if:
$$Q^T Q = Q Q^T = I$$ Therefore, a matrix $Q$ is orthogonal if:
$$Q^T = Q^{-1}$$ Jacobian $$ \mathbf J_{ij} = \frac{\partial f_{i}}{\partial x_{j}} = \begin{bmatrix} \frac{\partial f_{1}}{\partial x} &amp; \frac{\partial f_{1}}{\partial y} \\ \frac{\partial f_{2}}{\partial x} &amp; \frac{\partial f_{2}}{\partial y} \\ \end{bmatrix}$$ Example $$ \mathbf f(x,y) = \begin{bmatrix} x^2 y \\ 5x + \sin{y} \\ \end{bmatrix}$$ $$ \mathbf J = \begin{bmatrix} 2xy &amp; x^2 \\ 5 &amp; \cos{y} \\ \end{bmatrix} $$ Linear Regression Can use Least Squares with QR Decomposition.</description>
    </item>
    
  </channel>
</rss>